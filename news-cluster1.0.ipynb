{
 "metadata": {
  "name": "",
  "signature": "sha256:d9255efc544eaf79863cf5bd8267050fd08a9037aa32e2b7c3af168a4178bab3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'Jimin.Zhou'\n",
      "from pyspark import SparkContext\n",
      "from pyspark.serializers import PickleSerializer\n",
      "from pyspark import SparkConf,Row\n",
      "from pyspark.sql import SQLContext\n",
      "import logging\n",
      "from logging.config import fileConfig\n",
      "from datetime import *\n",
      "import traceback\n",
      "import ConfigParser\n",
      "import io\n",
      "import cPickle\n",
      "import sys\n",
      "import pymongo\n",
      "import mysql.connector\n",
      "from scipy.sparse import *\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from math import *\n",
      "from numpy import *\n",
      "import re\n",
      "from itertools import chain\n",
      "import pandas as pd\n",
      "import re\n",
      "from collections import defaultdict\n",
      "import itertools\n",
      "from scipy.cluster.hierarchy import *\n",
      "import jieba\n",
      "from scipy.sparse import csr_matrix\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getFeatVec(newsID, feat):\n",
      "    vec = []\n",
      "    for ft in feat:\n",
      "        cp = ft.split(',')\n",
      "        if len(cp) > 1:\n",
      "            first = cp[0].encode('utf-8')\n",
      "            second = cp[1].encode('utf-8')\n",
      "            if first.isdigit():\n",
      "                vec.append((newsID, int(first), float(second)))\n",
      "    return vec\n",
      "def agg(wd, ar):\n",
      "    arr = []\n",
      "    for n in ar:\n",
      "        for m in ar:\n",
      "            if n[0]<m[0]:\n",
      "                tt = n[1]*m[1]\n",
      "                if tt>0.003:\n",
      "                    arr.append(((n[0],m[0]),tt))\n",
      "    return arr\n",
      "def find(x):\n",
      "    global unset\n",
      "    p = x\n",
      "    while (unset[p]>=0):\n",
      "        p=unset[p]\n",
      "    while (x!=p):\n",
      "        t = unset[x]\n",
      "        unset[x] = p\n",
      "        x = t\n",
      "    return x\n",
      "def unionSet(x,y):\n",
      "    global unset\n",
      "    x = find(x)\n",
      "    y = find(y)\n",
      "    if (x == y):\n",
      "        return\n",
      "    if unset[x] < unset[y]:\n",
      "        unset[x] = unset[x] + unset[y]\n",
      "        unset[y] = x\n",
      "    else:\n",
      "        unset[y] = unset[y] + unset[x];\n",
      "        unset[x] = y\n",
      "import pymongo\n",
      "import re\n",
      "\n",
      "'''\n",
      "Created on 2013-11-7\n",
      "\n",
      "@author: guoxiong.yuan\n",
      "'''\n",
      "\n",
      "\n",
      "def mongo_find(col, query, projection=None, sort=None, limit=2147483647, batch_size=100):\n",
      "    cursor = col.find(spec=query, fields=projection, sort=sort, limit=limit)\n",
      "    if batch_size > 0:\n",
      "        cursor.batch_size(batch_size)\n",
      "    return cursor\n",
      "\n",
      "\n",
      "def mongo_find_one(col, query, projection=None, sort=None):\n",
      "    return col.find_one(spec_or_id=query, fields=projection, sort=sort)\n",
      "\n",
      "\n",
      "def mongo_save(col, obj):\n",
      "    col.save(obj);\n",
      "\n",
      "\n",
      "def mongo_update(col, query, document, multi=False):\n",
      "    if \"_id\" in query:\n",
      "        col.update({\"_id\": query[\"_id\"]}, document, multi=multi)\n",
      "    else:\n",
      "        col.update(query, document, multi=multi)\n",
      "\n",
      "\n",
      "def mongo_remove(col, doc):\n",
      "    if \"_id\" in doc:\n",
      "        col.remove({\"_id\": doc[\"_id\"]})\n",
      "    else:\n",
      "        col.remove(doc)\n",
      "\n",
      "\n",
      "def translate_filename(filename):\n",
      "    filename = re.sub(r'/|\\\\|\\\"|\\<|>|\\?|:', \"-\", filename)\n",
      "    filename = re.sub(r'\\s', \"\", filename)\n",
      "    filename = filename.replace('*', u'\uff0a')\n",
      "    return filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getNews(stDate, edDate):\n",
      "    global sqlContext\n",
      "    pirNews = sqlContext.sql(\"SELECT * FROM news WHERE publishDate > '\"+stDate+\"' AND publishDate < '\"+edDate+\"'\")\n",
      "    return pirNews"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def repartition(dataList,threashold):\n",
      "    global cosineDist\n",
      "    cc =cosineDist.todok()\n",
      "    see = set(dataList)\n",
      "    mat_dok = [elm for elm in cc.iteritems() if (elm[0][0] in see)|(elm[0][1] in see)]\n",
      "    noZeroInd = [elm[0] for elm in mat_dok if elm[1]>threashold]\n",
      "    return noZeroInd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cluster(dataList):\n",
      "    global cosineDist\n",
      "    global newsCatRes\n",
      "    global cateIndex\n",
      "    distMat = cosineDist[dataList][:,dataList]\n",
      "    ind = list(itertools.combinations(range(len(dataList)),2))\n",
      "    distMat = 1.0- distMat.todense()\n",
      "    distMat[distMat<0]=0.0\n",
      "    distMatCon = [distMat[ii] for ii in ind]\n",
      "    linkJ = average(distMatCon)\n",
      "    clusterJ = fcluster(linkJ, 0.7, 'distance')\n",
      "    cll = len(clusterJ)\n",
      "    for ind in xrange(cll):\n",
      "       # news_cls_res[dataList[ind]]=clusterJ[ind]*tol+indx\n",
      "        newsCatRes[dataList[ind]]=cateIndex-1+clusterJ[ind]\n",
      "    cateIndex = cateIndex+max(clusterJ)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def directCombine(dataList):\n",
      "    global cateIndex\n",
      "    ll = len(dataList)\n",
      "    for ind in xrange(ll):\n",
      "       # news_cls_res[dataList[ind]]=clusterJ[ind]*tol+indx\n",
      "        newsCatRes[dataList[ind]]=cateIndex\n",
      "    cateIndex = cateIndex+1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def maxDate(nws):\n",
      "    return max([x.publishDate for x in nws])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def groupNewsRes(news, groupMap):\n",
      "    if groupMap.has_key(news.newsID):\n",
      "        return (groupMap[news.newsID], news)\n",
      "    else:\n",
      "        return (news.newsID, news)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def partition(dataList, c):\n",
      "    global unset\n",
      "    #group to partitions\n",
      "    noZeroInd = repartition(dataList, c)\n",
      "    indexMap = sorted(list(set(chain.from_iterable(noZeroInd))))\n",
      "    filteredNum = len(indexMap)\n",
      "    map2Index = dict(zip(indexMap,range(filteredNum)))\n",
      "    unset = [-1]*filteredNum\n",
      "    for elm in noZeroInd:\n",
      "        if elm[0] < elm[1]:\n",
      "            unionSet(map2Index[elm[0]], map2Index[elm[1]])\n",
      "    nodeLocMap = {}\n",
      "    for ind in range(filteredNum):\n",
      "        nodeLocMap[indexMap[ind]] = find(ind)\n",
      "    groupedDistMat = defaultdict(list)\n",
      "    for key, value in sorted(nodeLocMap.iteritems()):\n",
      "        groupedDistMat[value].append(key)\n",
      "    return groupedDistMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def triple2ParseMatrix(finalData):\n",
      "    #get dist mat to driver and regularize\n",
      "    DistTriple = finalData.filter(lambda (x,y):y>0.3).collect()\n",
      "    DistI = [int(x) for ((x,y),z) in DistTriple]\n",
      "    DistJ = [int(y) for ((x,y),z) in DistTriple]\n",
      "    if len(DistI) == 0:\n",
      "        maxInd = -1;\n",
      "    maxInd = max(DistI+DistJ)\n",
      "    DistV = [z for ((x,y),z) in DistTriple]\n",
      "    Dist = coo_matrix((DistV,(DistI,DistJ)),shape=(maxInd+1,maxInd+1))\n",
      "    Dist = Dist.tocsr()\n",
      "    return Dist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#incursive partitions\n",
      "def incurCluster(dataList, c):\n",
      "    groupedDistMat = partition(dataList, c)\n",
      "    #cluster by partiotions\n",
      "    cc = 0\n",
      "    print len(groupedDistMat)\n",
      "    for item in groupedDistMat.iteritems():\n",
      "        dataList = item[1]\n",
      "        #print len(dataList)\n",
      "        if len(dataList) < 5:\n",
      "            cc = cc + 1\n",
      "            directCombine(dataList)\n",
      "            continue\n",
      "            \n",
      "        if len(dataList) > 500:\n",
      "            #print len(dataList)\n",
      "            incurCluster(dataList, c+0.1)\n",
      "        else:\n",
      "            cluster(dataList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def processPriodNews(pirNews):\n",
      "    global cosineDist\n",
      "    global newsCatRes\n",
      "    global cateIndex\n",
      "    global distList\n",
      "    #get dist pair\n",
      "    newsFtData = pirNews.flatMap(lambda news: getFeatVec(news.newsID, news.newsFeature))\n",
      "    dataGroupped = newsFtData.filter(lambda (x,y,z):z>0.01).map(lambda (x,y,z):(y,(x,z))).groupByKey()\n",
      "    finalData = dataGroupped.flatMap(lambda (x,y):agg(x,y)).reduceByKey(lambda x,y:x+y)\n",
      "    #get parse matrix\n",
      "    cosineDist = triple2ParseMatrix(finalData)\n",
      "    distDok =cosineDist.todok().keys()\n",
      "    #partition\n",
      "    newsCatRes = {}\n",
      "    distList = sorted(list(set(chain.from_iterable(distDok))))\n",
      "    cateIndex = 0\n",
      "    print str(time.localtime().tm_min) + \"starting cluster , news news count:\" + str(len(distList))\n",
      "    incurCluster(distList,0.3)\n",
      "    print str(time.localtime().tm_min) + \"cluster end\"\n",
      "    catedNewsMat = defaultdict(list)\n",
      "    for key, value in sorted(newsCatRes.iteritems()):\n",
      "        catedNewsMat[value].append(key)\n",
      "    #build news-clusterID map\n",
      "    newsCateFinalRes = {}\n",
      "    for gp,key in catedNewsMat.iteritems():\n",
      "        minID = inf\n",
      "        for item in key:\n",
      "            if minID > item:\n",
      "                minID = item\n",
      "        for item in key:\n",
      "            newsCateFinalRes[item]=minID\n",
      "    #broadcast group res\n",
      "    newsCateResRDD = sc.broadcast(newsCateFinalRes)\n",
      "    #save result(groupID and news) in RDD\n",
      "    catResAllRDD = pirNews.map(lambda nws: groupNewsRes(nws, newsCateResRDD.value))\n",
      "    catedAllRDD = catResAllRDD.groupByKey()\n",
      "    return catedAllRDD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def restart(curDate):\n",
      "    print \"restarting...\"\n",
      "    global sc\n",
      "    global schemaNews\n",
      "    global sqlContext\n",
      "    global activeNewsRDD\n",
      "    global conf\n",
      "    dataTosave = activeNewsRDD.collect()\n",
      "    output_file = open(curDate.strftime('%Y-%m-%d')+\"_activeNewsRDD.pickle\",'w')  \n",
      "    cPickle.dump(dataTosave,output_file)\n",
      "    output_file.close()\n",
      "    sc.stop()\n",
      "    sc = SparkContext(conf=conf)\n",
      "    allFiles = sc.textFile('/dw/sparkteam/jimin.zhou/JLnewsFeature201409252/*')\n",
      "    sqlContext = SQLContext(sc)\n",
      "    parts = allFiles.map(lambda l: l.split(\"\\t\"))\n",
      "    newsFeatures = parts.map(lambda p: {\"publishDate\": p[0], \"newsID\": long(p[1]), \"newsFeature\":p[2].strip().split(' ')})\n",
      "    schemaNews = sqlContext.inferSchema(newsFeatures)\n",
      "    schemaNews.registerAsTable(\"news\")\n",
      "    activeNewsRDD = sc.parallelize(dataTosave)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def saveToMongo(groupedAllRDD):\n",
      "    global mongodb\n",
      "    global mongo_base\n",
      "    res = groupedAllRDD.collect()\n",
      "    for elm in res:\n",
      "        for news in elm[1]:\n",
      "            mongo_save(mongodb[mongo_base].jl_news_cluster, {'_id':news.newsID,'pubT':news.publishDate, 'sid':int(elm[0])})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def processDayAcc(stDate, edDate):\n",
      "    #get news active\n",
      "    global activeNewsRDD\n",
      "    curDate = stDate\n",
      "    while curDate < edDate:\n",
      "        if curDate.isoweekday()==7:\n",
      "            restart(curDate)\n",
      "        edDateTmp = curDate + timedelta(3)\n",
      "        stDateStr = curDate.strftime('%Y-%m-%d')\n",
      "        edDateStr = edDateTmp.strftime('%Y-%m-%d')\n",
      "        print str(time.localtime().tm_min) + \"start processing + \" + stDateStr\n",
      "        #cal active\n",
      "        tempRDD = activeNewsRDD.filter(lambda x: x.publishDate < stDateStr)\n",
      "        pirNews = getNews(stDateStr,edDateStr)\n",
      "        pirNews = pirNews.union(tempRDD)\n",
      "        groupedAllRDD = processPriodNews(pirNews)\n",
      "        nextDate = curDate + timedelta(1)\n",
      "        nextDateStr = nextDate.strftime('%Y-%m-%d')\n",
      "        res = groupedAllRDD.map(lambda (x,y): (x,y,maxDate(y))).filter(lambda (x,y,z): z>nextDateStr)\n",
      "        activeNewsRDD = res.flatMap(lambda (x,y,z):list(y))\n",
      "        print str(time.localtime().tm_min) + \" starting save to mongoDB\"\n",
      "        saveToMongo(groupedAllRDD)\n",
      "        curDate = curDate + timedelta(1)       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def processDayFirst7(stDate):\n",
      "    #get news active\n",
      "    global activeNewsRDD\n",
      "    global pirNews\n",
      "    edDateTmp = stDate + timedelta(3)\n",
      "    stDateStr = stDate.strftime('%Y-%m-%d')\n",
      "    edDateStr = edDateTmp.strftime('%Y-%m-%d')\n",
      "    pirNews = getNews(stDateStr,edDateStr)\n",
      "    groupedAllRDD = processPriodNews(pirNews)\n",
      "    saveToMongo(groupedAllRDD)\n",
      "    nextDate = stDate + timedelta(1)\n",
      "    nextDateStr = nextDate.strftime('%Y-%m-%d')\n",
      "    res = groupedAllRDD.map(lambda (x,y): (x,y,maxDate(y))).filter(lambda (x,y,z): z>nextDateStr)\n",
      "    activeNewsRDD = res.flatMap(lambda (x,y,z):list(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def init():\n",
      "    global sc\n",
      "    global sqlContext\n",
      "    global schemaNews\n",
      "    global conf\n",
      "    sc = SparkContext(conf=conf)\n",
      "    allFiles = sc.textFile('/dw/sparkteam/jimin.zhou/JLnewsFeature201409252/*')\n",
      "    sqlContext = SQLContext(sc)\n",
      "    parts = allFiles.map(lambda l: l.split(\"\\t\"))\n",
      "    newsFeatures = parts.map(lambda p: {\"publishDate\": p[0], \"newsID\": long(p[1]), \"newsFeature\":p[2].strip().split(' ')})\n",
      "    schemaNews = sqlContext.inferSchema(newsFeatures)\n",
      "    schemaNews.registerAsTable(\"news\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def restoreFromError(curDate, endDate):\n",
      "    print \"loading and restarting...\"\n",
      "    global sc\n",
      "    global activeNewsRDD\n",
      "    output_file = file(curDate.strftime('%Y-%m-%d')+\"_activeNewsRDD.pickle\")  \n",
      "    dataTosave = cPickle.load(output_file)\n",
      "    output_file.close()\n",
      "    activeNewsRDD = sc.parallelize(dataTosave)\n",
      "    processDayAcc(curDate, endDate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main__():\n",
      "    #batch all\n",
      "    global conf\n",
      "    global mongodb\n",
      "    global mongo_base\n",
      "    mongo_url = \"mongodb://app_mole_dev:SOh3TbYhxuLiW8@nosql05-dev.datayes.com/mole-dev\"\n",
      "    mongo_base = \"mole-dev\"\n",
      "    mongodb = pymongo.MongoClient(mongo_url)\n",
      "    conf = SparkConf()\n",
      "    conf.setMaster(\"yarn-client\").setAppName(\"hirachical_sql\").set(\"spark.executor.instances\", 9)\n",
      "    init()\n",
      "    #batch\n",
      "    processDayFirst7(datetime(2013,1,1))\n",
      "    #processDayAcc(datetime(2010,1,2), datetime(2014,9,26))\n",
      "    #restoreFromError(datetime(2012,12,30),datetime(2014,9,11))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main__()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "--args is deprecated. Use --arg instead.\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o86.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4.0:12 failed 4 times, most recent failure: TID 59 on host sh-prd-hadoop-dn03 failed for unknown reason\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-b9ca858a92d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-20-2f94af6c9924>\u001b[0m in \u001b[0;36mmain__\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprocessDayFirst7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2013\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#processDayAcc(datetime(2010,1,2), datetime(2014,9,26))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#restoreFromError(datetime(2012,12,30),datetime(2014,9,11))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-17-9fb53db2be92>\u001b[0m in \u001b[0;36mprocessDayFirst7\u001b[1;34m(stDate)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0medDateStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medDateTmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpirNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstDateStr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medDateStr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mgroupedAllRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessPriodNews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpirNews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0msaveToMongo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroupedAllRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnextDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstDate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-13-d3c614829cf1>\u001b[0m in \u001b[0;36mprocessPriodNews\u001b[1;34m(pirNews)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfinalData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataGroupped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#get parse matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcosineDist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriple2ParseMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinalData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdistDok\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mcosineDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodok\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-11-e006a60ea53c>\u001b[0m in \u001b[0;36mtriple2ParseMatrix\u001b[1;34m(finalData)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtriple2ParseMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinalData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#get dist mat to driver and regularize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mDistTriple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mDistI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDistTriple\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mDistJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDistTriple\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/datayes/data/jimin.zhou/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \"\"\"\n\u001b[0;32m    582\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m           \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/datayes/data/jimin.zhou/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 537\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/datayes/data/jimin.zhou/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o86.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4.0:12 failed 4 times, most recent failure: TID 59 on host sh-prd-hadoop-dn03 failed for unknown reason\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}